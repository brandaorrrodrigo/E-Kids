# ğŸ¤– E-Kids PRO - Chat Server (Isolado)

Servidor isolado que roda **apenas** a API de chat com Ollama + TTS.

---

## ğŸ¯ Por que servidor isolado?

Esta arquitetura separa o chat do resto do sistema:

- **Servidor Principal (porta 3000)**: Vai para Railway/Vercel (nuvem)
  - App web, autenticaÃ§Ã£o, database, jogos, etc.

- **Servidor de Chat (porta 3001)**: Roda no seu PC local
  - Ollama (IA local)
  - TTS (gTTS - voz feminina)
  - Exposto via Cloudflare Tunnel (HTTPS grÃ¡tis)

---

## ğŸ“¦ InstalaÃ§Ã£o

```bash
cd server-chat
npm install
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

Crie/edite o arquivo `.env`:

```env
PORT=3001
API_KEY=ekids-chat-secret-key-2025-ultra-secure
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
RATE_LIMIT_MAX=20
RATE_LIMIT_WINDOW_MS=60000
```

---

## â–¶ï¸ Executar

### Windows:

```bat
start-chat-server.bat
```

### Linux/Mac:

```bash
npm start
```

---

## ğŸ”’ SeguranÃ§a

### API Key

Todas as requisiÃ§Ãµes precisam do header:

```
X-API-Key: ekids-chat-secret-key-2025-ultra-secure
```

### Rate Limiting

MÃ¡ximo de **20 requisiÃ§Ãµes por minuto** por IP/usuÃ¡rio.

---

## ğŸ“¡ Endpoints

### GET /health
Verifica status do servidor (sem autenticaÃ§Ã£o).

```bash
curl http://localhost:3001/health
```

### GET /api/chat/health
Verifica se Ollama estÃ¡ online.

```bash
curl -H "X-API-Key: sua-key" http://localhost:3001/api/chat/health
```

### POST /api/chat
Envia mensagem para o chatbot.

```bash
curl -X POST http://localhost:3001/api/chat \
  -H "X-API-Key: sua-key" \
  -H "Content-Type: application/json" \
  -d '{
    "childName": "JoÃ£o",
    "message": "Oi Lu!",
    "contextType": "general"
  }'
```

Resposta:
```json
{
  "success": true,
  "message": "Oi JoÃ£o! Como vocÃª estÃ¡ hoje? ğŸ˜Š",
  "tokens": 25,
  "model": "llama3.2:latest"
}
```

### POST /api/tts/speak
Gera Ã¡udio com voz feminina (gTTS).

```bash
curl -X POST http://localhost:3001/api/tts/speak \
  -H "X-API-Key: sua-key" \
  -H "Content-Type: application/json" \
  -d '{"text": "OlÃ¡! Eu sou a Lu!"}' \
  --output audio.mp3
```

---

## ğŸŒ Expor na Internet

Use **Cloudflare Tunnel** para expor este servidor:

```bash
# Modo temporÃ¡rio (teste)
cloudflared tunnel --url http://localhost:3001

# Modo permanente (ver CLOUDFLARE_TUNNEL_SETUP.md)
cloudflared tunnel run --config cloudflared-config.yml ekids-chat
```

---

## ğŸ” Logs

O servidor loga todas as requisiÃ§Ãµes:

```
2025-12-22T19:30:45.123Z - POST /api/chat
ğŸ’¬ Chat request: JoÃ£o - "Oi Lu!"
```

---

## ğŸ› Troubleshooting

### Erro: "Ollama nÃ£o estÃ¡ rodando"

```bash
# Inicie o Ollama em outro terminal
ollama serve
```

### Erro: "API key invÃ¡lida"

Verifique se a API_KEY no `.env` do servidor de chat Ã© a **mesma** que a CHAT_API_KEY no `.env` do servidor principal.

### Porta 3001 jÃ¡ em uso

```bash
# Windows: encontrar processo
netstat -ano | findstr :3001

# Matar processo (substitua PID)
taskkill /F /PID 12345

# Linux/Mac
lsof -ti:3001 | xargs kill -9
```

---

## ğŸ“Š Arquitetura Completa

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Railway/Vercel (Nuvem)                 â”‚
â”‚  â”œâ”€ Servidor Principal (port 3000)     â”‚
â”‚  â”œâ”€ App Web, DB, Auth                   â”‚
â”‚  â””â”€ Proxy para Chat Server              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTPS (API Key)
               â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Cloudflare Tunnel     â”‚
     â”‚  chat.ekidspro.com     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Seu PC (localhost)                     â”‚
â”‚  â””â”€ Chat Server (port 3001) â—„â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”œâ”€ API /api/chat                â”‚   â”‚
â”‚     â””â”€ TTS /api/tts                 â”‚   â”‚
â”‚                                     â”‚   â”‚
â”‚  â””â”€ Ollama (port 11434) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚     â””â”€ llama3.2:latest                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Checklist

- [ ] Node.js instalado
- [ ] Ollama instalado e rodando
- [ ] Modelo llama3.2:latest baixado
- [ ] Python + gTTS instalado
- [ ] npm install executado
- [ ] .env configurado
- [ ] Servidor iniciado (npm start)
- [ ] Teste de health funcionando
- [ ] Cloudflare Tunnel configurado (opcional)

---

**Pronto para receber requisiÃ§Ãµes do servidor principal!** ğŸš€
